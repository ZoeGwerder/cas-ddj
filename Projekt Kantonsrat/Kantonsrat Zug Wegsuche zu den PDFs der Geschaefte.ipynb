{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL von allen Hauptseiten holen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alle urls und infos der Hauptseiten holen.\n",
    "url = \"https://kr-geschaefte.zug.ch/gast/geschaefte?commit=Filtern&geschaeft_filter%5Babgeschlossen_bis%5D=&geschaeft_filter%5Babgeschlossen_von%5D=&geschaeft_filter%5Barten_refs%5D%5B%5D=&geschaeft_filter%5Beingereicht_bis%5D=&geschaeft_filter%5Beingereicht_von%5D=&geschaeft_filter%5Bfrist_bis%5D=&geschaeft_filter%5Bhistorische_staende_refs%5D%5B%5D=&geschaeft_filter%5Bkommissionen_refs%5D%5B%5D=&geschaeft_filter%5Bstaende_refs%5D%5B%5D=&geschaeft_filter%5Bstatus_ids%5D%5B%5D=haengig&geschaeft_filter%5Bstatus_ids%5D%5B%5D=abgeschlossen&geschaeft_filter%5Bstatus_ids%5D%5B%5D=&geschaeft_filter%5Btitel%5D=&geschaeft_filter%5Bzustaendig_refs%5D%5B%5D=&page=\"\n",
    "\n",
    "alle_seiten = []\n",
    "for seite in range(1,41):\n",
    "    r=requests.get(url+str(seite))\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "    \n",
    "    tr_list=soup.find_all(\"tr\")[2:] #erst ab der Position 2 sind die Daten relevant.\n",
    "    \n",
    "    for element in tr_list:\n",
    "        td_list = element.find_all(\"td\")\n",
    "        vorlag_nr=td_list[0].text\n",
    "        href = td_list[0].find(\"a\")[\"href\"]\n",
    "        text = td_list[1].text\n",
    "        stand = td_list[2].text\n",
    "        zustaendig = td_list[3].text\n",
    "        geschaeft_art = td_list[4].text\n",
    "        status = td_list[5].text\n",
    "        \n",
    "        mini_dict = {\"GeschäftsNR\":vorlag_nr,\"url\":href,\"Geschäft\":text, \"Stand\":stand, \"Zuständigkeit\":zustaendig, \"Art des Geschäfts\": geschaeft_art, \"Status\":status}\n",
    "\n",
    "        alle_seiten.append(mini_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(alle_seiten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a=pd.DataFrame(alle_seiten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eine Liste aus allen Geschäfts-URLs machen (um später auf diese zuzugreiffen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_liste_ganz=[]\n",
    "url_anfang_original=\"https://kr-geschaefte.zug.ch\"\n",
    "for element in df_a[\"url\"]:\n",
    "    url_liste_ganz.append(url_anfang_original+element)\n",
    "#diese werden zusammengefügt aus dem Anfang der Adresse sowie dem Geschäfts-URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die einzelnen Geschäfte aufrufen und Infos rausholen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einschub: ich will speichern und mit diesen Daten dann arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#habe nun alle Seiten der Geschäfte auf meinem Rechner gespeichert. \n",
    "number = 0\n",
    "for page in url_liste_ganz:\n",
    "    page_content = requests.get(page)\n",
    "    page_content = page_content.text\n",
    "    with open(\"Geschaeft\"+str(number)+\".html\", \"w\") as file:\n",
    "        file.write(page_content)\n",
    "        file.close()\n",
    "    number +=1\n",
    "    #Es sind nun exakt so viele Seiten auf meinem Rechner, wie das DataFrame oben Zeilen anzeigt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einschub-Ende. Ich habe es geschafft, das zu speichern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nun muss ich den Code ändern, sodass ich nicht mehr online zugreiffe sondern über meinen Ordner.\n",
    "#hier nun die Liste der Zugriffe.\n",
    "geschaefts_liste=[]#####ACHTUNG ich mache den RANGE kleiner, um weniger GEschäfte zu erhalten.\n",
    "\n",
    "for seite_g in range(0,1963):\n",
    "    gesch= \"KRhtml/Geschaeft\"+str(seite_g)+\".html\"\n",
    "    geschaefts_liste.append(gesch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geschaefts_liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nun seite für Seite abrufen und in einem neuen Minidict unterbringen\n",
    "#ACHTUNG: es bringt zumindest für einige die falschen Daten raus. und dann gleich mehrfach. \n",
    "#(Geschäft: 2129 8fach. Nur das richtige Datum ist nicht drinn.)\n",
    "eingang_list=[]\n",
    "geschaeft= \"KRhtml/Geschaeft61.html\"\n",
    "\n",
    "file = open(geschaeft, 'r')\n",
    "text = file.read()\n",
    "soup_g = BeautifulSoup(text, 'html.parser')\n",
    "tr_g_list=soup_g.find_all(\"tr\")\n",
    "td_g_list=tr_g_list[-1].find_all(\"td\")#hier gehe ich zum Dokument. Dieses scheint von unten her an derselben Position zu sein\n",
    "url_v_pdf = td_g_list[3].find(\"a\")[\"href\"] #nun hier die URL zum PDF des Vorstosses\n",
    "gesch_nr = tr_g_list[-1].find_all(\"td\")[0].text\n",
    "einger_am = tr_g_list[1].find_all(\"td\")[0].text   \n",
    "ul_list = tr_g_list[2].find_all(\"ul\")\n",
    "li_list = ul_list[0].find_all(\"li\")\n",
    "    \n",
    "for url in li_list:\n",
    "    urlna = url.find(\"a\")[\"href\"]\n",
    "    name = url.find(\"a\").text\n",
    "    \n",
    "#So hole ich nun all die vorlagen Pdfs. Nur: wenn vier namen dort stehen, wird es auch vier Mal aufgelistet. \n",
    "\n",
    "    \n",
    "    minidict_g={\"GeschäftsNR\":gesch_nr, \"Einreichedatum\":einger_am, \"Namens-url\": urlna, \"Name eingereicht\":name, \"Link Vorstoss-PDF\":url_v_pdf}\n",
    "    for key, value in minidict_g.items():\n",
    "        if value == '':\n",
    "            minidict_g[key] = 'NaN'  #hier schaue ich noch, dass ich die Leeren Zeilen mit NAN ersetzten kann.\n",
    "        else:\n",
    "            minidict_g[key] = value\n",
    "            \n",
    "    eingang_list.append(minidict_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hier versuche ich, ein separates Dataframe zu erstellen, mit VorlagenNR und url zum PDF der Vorlage\n",
    "#### Da sonst jedes PDF vierfach angesteuert wird. \n",
    "Ich will so nun ein separates Dataframe generieren, in welchem ich dann auch die Einreichedaten und Namen aus den PDFs unterbringe. und es erst später dann mit den anderen Daten zusammenfüge. Sonst denke ich, gibts ein Puff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eingang_list=[]\n",
    "geschaeft= \"KRhtml/Geschaeft61.html\"\n",
    "\n",
    "file = open(geschaeft, 'r')\n",
    "text = file.read()\n",
    "soup_g = BeautifulSoup(text, 'html.parser')\n",
    "tr_g_list=soup_g.find_all(\"tr\")\n",
    "td_g_list=tr_g_list[-1].find_all(\"td\")#hier gehe ich zum Dokument. Dieses scheint von unten her an derselben Position zu sein\n",
    "url_v_pdf = td_g_list[3].find(\"a\")[\"href\"] #nun hier die URL zum PDF des Vorstosses\n",
    "gesch_nr = tr_g_list[-1].find_all(\"td\")[0].text\n",
    "einger_am = tr_g_list[1].find_all(\"td\")[0].text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=pd.DataFrame(eingang_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=pd.DataFrame(eingang_list)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Das Datum muss noch als solches lesbar gemacht werden\n",
    "df_g[\"Einreichedatum\"] =  pd.to_datetime(df_g[\"Einreichedatum\"], format='%d.%m.%Y') #verwandle string in datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mit_dat=pd.merge(df_a, df_g, how=\"left\", on=\"GeschäftsNR\") #ich verbinde die beiden DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nun will ich noch das Datum zum Idex machen\n",
    "df_mit_dat.set_index(\"Einreichedatum\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mit_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ein erstes Ergebnis = ab 2013 alle  Vorstösse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nun will ich ein erstes Mal in die DAten schauen, zumindest ab 2013 - ob es ein Muster beim Einreichen gibt.\n",
    "#DAzu krame ich jetzt alle Vorstösse raus\n",
    "df_vo_all=df_mit_dat[{'Art des Geschäfts' : [\"Motion\",\"Interpellation\",\"Postulat\",\"Kleine Anfrage\"] }] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vo_all[\"2013\":].resample(\"SMS\").count().plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Baby! \n",
    "### Ich versuche mit Regex in Pandas an die Namen zu kommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nun versuche ich mit Regex an die einzelnen Namen zu kommen.\n",
    "regex_n=r\"\\bvon(.*und.\\w*.\\w*\\b)\"\n",
    "regex_n2= r\"^(\\w*.\\w*\\b)\\,.(\\w*.\\w*\\b)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n1=df_a['Geschäft'].str.extract(regex_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
